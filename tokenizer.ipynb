{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2bf686",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1722d",
   "metadata": {},
   "source": [
    "Step 1: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b650555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data[1]:  This document is the second document.\n"
     ]
    }
   ],
   "source": [
    "# Import the corpus data from the text file \n",
    "\n",
    "# corpus = []\n",
    "# with open('D:\\\\dev\\\\Llama_from_scrtach\\\\Reviews.txt', 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         corpus.append(line.strip())\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "print(\"Training data[1]: \",corpus[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5dddcb",
   "metadata": {},
   "source": [
    "Step 2: Initialize and Pre-tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef26e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  [' ', '.', '?', 'A', 'I', 'T', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u', '</w>']\n",
      "Vocabulary size:  20\n"
     ]
    }
   ],
   "source": [
    "# add the chars to a set to get unique chars\n",
    "unique_chars = set()\n",
    "for line in corpus:\n",
    "    for char in line:\n",
    "        unique_chars.add(char)\n",
    "        \n",
    "# sort        \n",
    "vocab = list(unique_chars)\n",
    "vocab.sort()\n",
    "vocab.append('</w>')  # end of word token\n",
    "print(\"Vocabulary: \",vocab)\n",
    "print(\"Vocabulary size: \",len(vocab))\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3772dbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenized Word Frequencies:  {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "Pre-tokenized Word Frequencies size:  13\n"
     ]
    }
   ],
   "source": [
    "word_splits = {}\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    for word in words:\n",
    "        if word:\n",
    "            chars_list = list(word) + ['</w>']\n",
    "            # Tuple for immutability\n",
    "            word_tuple = tuple(chars_list)\n",
    "            if word_tuple not in word_splits:\n",
    "                word_splits[word_tuple] = 0\n",
    "            word_splits[word_tuple] += 1\n",
    "print(\"Pre-tokenized Word Frequencies: \",word_splits)\n",
    "print(\"Pre-tokenized Word Frequencies size: \",len(word_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf74ea",
   "metadata": {},
   "source": [
    "Helper Function ```get_pair_stats```\n",
    "find the most frequent pair of symbols in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67679928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs Counts:  defaultdict(<class 'int'>, {('T', 'h'): 2, ('h', 'i'): 5, ('i', 's'): 7, ('s', '</w>'): 8, ('t', 'h'): 7, ('h', 'e'): 4, ('e', '</w>'): 4, ('f', 'i'): 2, ('i', 'r'): 3, ('r', 's'): 2, ('s', 't'): 2, ('t', '</w>'): 3, ('d', 'o'): 4, ('o', 'c'): 4, ('c', 'u'): 4, ('u', 'm'): 4, ('m', 'e'): 4, ('e', 'n'): 4, ('n', 't'): 4, ('t', '.'): 2, ('.', '</w>'): 3, ('s', 'e'): 1, ('e', 'c'): 1, ('c', 'o'): 1, ('o', 'n'): 2, ('n', 'd'): 2, ('d', '</w>'): 3, ('A', 'n'): 1, ('r', 'd'): 1, ('n', 'e'): 1, ('e', '.'): 1, ('I', 's'): 1, ('t', '?'): 1, ('?', '</w>'): 1})\n"
     ]
    }
   ],
   "source": [
    "import collections \n",
    "\n",
    "def get_pair_stats(splits):\n",
    "    \"\"\"Get the frequency of all pairs of adjacent symbols in the splits.\"\"\"\n",
    "    pairs_counts = collections.defaultdict(int)\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pairs_counts[pair] += freq\n",
    "    return pairs_counts\n",
    "print(\"Pairs Counts: \",get_pair_stats(word_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09f208",
   "metadata": {},
   "source": [
    "Helper Function ```merge_pair```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a34c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Pair:  ('s', '</w>')\n"
     ]
    }
   ],
   "source": [
    "def merge_pair(pair_to_merge, splits):\n",
    "    \"\"\"Merge the specified pair in the word splits.\"\"\"\n",
    "    new_splits = {}\n",
    "    (first, second) = pair_to_merge\n",
    "    merged_token = first + second\n",
    "    for word_tuple, freq in splits.items():\n",
    "        symbols = list(word_tuple)\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and symbols[i] == first and symbols[i + 1] == second:\n",
    "                new_symbols.append(merged_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_splits[tuple(new_symbols)] = freq\n",
    "    return new_splits\n",
    "\n",
    "def get_most_frequent_pair(pairs_counts):\n",
    "    \"\"\"\n",
    "        Get the most frequent pair from the pairs counts.\n",
    "    \"\"\"\n",
    "    return max(pairs_counts, key=pairs_counts.get)\n",
    "\n",
    "print(\"Most Frequent Pair: \",get_most_frequent_pair(get_pair_stats(word_splits)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bc1340",
   "metadata": {},
   "source": [
    "Step 3: Iterative BPE Merging Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fc39576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Word Splits: {('T', 'h', 'i', 's', '</w>'): 2, ('i', 's', '</w>'): 3, ('t', 'h', 'e', '</w>'): 4, ('f', 'i', 'r', 's', 't', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '.', '</w>'): 2, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '</w>'): 1, ('s', 'e', 'c', 'o', 'n', 'd', '</w>'): 1, ('A', 'n', 'd', '</w>'): 1, ('t', 'h', 'i', 's', '</w>'): 2, ('t', 'h', 'i', 'r', 'd', '</w>'): 1, ('o', 'n', 'e', '.', '</w>'): 1, ('I', 's', '</w>'): 1, ('d', 'o', 'c', 'u', 'm', 'e', 'n', 't', '?', '</w>'): 1}\n",
      "\n",
      " Merge Iteration 1/ 15\n",
      "Top 5 pairs:  [(('s', '</w>'), 8), (('i', 's'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4)]\n",
      "Best pair: ('s', '</w>') with frequency: 8\n",
      "\n",
      " Merge Iteration 2/ 15\n",
      "Top 5 pairs:  [(('i', 's</w>'), 7), (('t', 'h'), 7), (('h', 'i'), 5), (('h', 'e'), 4), (('e', '</w>'), 4)]\n",
      "Best pair: ('i', 's</w>') with frequency: 7\n",
      "\n",
      " Merge Iteration 3/ 15\n",
      "Top 5 pairs:  [(('t', 'h'), 7), (('h', 'is</w>'), 4), (('h', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4)]\n",
      "Best pair: ('t', 'h') with frequency: 7\n",
      "\n",
      " Merge Iteration 4/ 15\n",
      "Top 5 pairs:  [(('th', 'e'), 4), (('e', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4)]\n",
      "Best pair: ('th', 'e') with frequency: 4\n",
      "\n",
      " Merge Iteration 5/ 15\n",
      "Top 5 pairs:  [(('the', '</w>'), 4), (('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4)]\n",
      "Best pair: ('the', '</w>') with frequency: 4\n",
      "\n",
      " Merge Iteration 6/ 15\n",
      "Top 5 pairs:  [(('d', 'o'), 4), (('o', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4)]\n",
      "Best pair: ('d', 'o') with frequency: 4\n",
      "\n",
      " Merge Iteration 7/ 15\n",
      "Top 5 pairs:  [(('do', 'c'), 4), (('c', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4)]\n",
      "Best pair: ('do', 'c') with frequency: 4\n",
      "\n",
      " Merge Iteration 8/ 15\n",
      "Top 5 pairs:  [(('doc', 'u'), 4), (('u', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4)]\n",
      "Best pair: ('doc', 'u') with frequency: 4\n",
      "\n",
      " Merge Iteration 9/ 15\n",
      "Top 5 pairs:  [(('docu', 'm'), 4), (('m', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3)]\n",
      "Best pair: ('docu', 'm') with frequency: 4\n",
      "\n",
      " Merge Iteration 10/ 15\n",
      "Top 5 pairs:  [(('docum', 'e'), 4), (('e', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3)]\n",
      "Best pair: ('docum', 'e') with frequency: 4\n",
      "\n",
      " Merge Iteration 11/ 15\n",
      "Top 5 pairs:  [(('docume', 'n'), 4), (('n', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3)]\n",
      "Best pair: ('docume', 'n') with frequency: 4\n",
      "\n",
      " Merge Iteration 12/ 15\n",
      "Top 5 pairs:  [(('documen', 't'), 4), (('i', 'r'), 3), (('t', '</w>'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3)]\n",
      "Best pair: ('documen', 't') with frequency: 4\n",
      "\n",
      " Merge Iteration 13/ 15\n",
      "Top 5 pairs:  [(('i', 'r'), 3), (('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2)]\n",
      "Best pair: ('i', 'r') with frequency: 3\n",
      "\n",
      " Merge Iteration 14/ 15\n",
      "Top 5 pairs:  [(('.', '</w>'), 3), (('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2)]\n",
      "Best pair: ('.', '</w>') with frequency: 3\n",
      "\n",
      " Merge Iteration 15/ 15\n",
      "Top 5 pairs:  [(('d', '</w>'), 3), (('T', 'h'), 2), (('h', 'is</w>'), 2), (('f', 'ir'), 2), (('ir', 's'), 2)]\n",
      "Best pair: ('d', '</w>') with frequency: 3\n"
     ]
    }
   ],
   "source": [
    "num_merges = 15\n",
    "\n",
    "merges = {}\n",
    "\n",
    "current_splits = word_splits.copy()\n",
    "\n",
    "print(f\"Initial Word Splits: {current_splits}\")\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(f\"\\n Merge Iteration {i + 1}/ {num_merges}\")\n",
    "    \n",
    "    \n",
    "    # Calculate pair frequency \n",
    "    pair_stats = get_pair_stats(current_splits)\n",
    "    if not pair_stats:\n",
    "        print(\"No more pairs to merge.\")\n",
    "        break\n",
    "    \n",
    "    # Print top 5 pairs for inspection\n",
    "    sorted_pairs = sorted(pair_stats.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(\"Top 5 pairs: \", sorted_pairs[:5])\n",
    "    \n",
    "    # Find the best pair \n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    best_freq = pair_stats[best_pair]\n",
    "    print(f\"Best pair: {best_pair} with frequency: {best_freq}\")\n",
    "    \n",
    "    # Merge the best pair\n",
    "    current_splits = merge_pair(best_pair, current_splits)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
